{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f6cb5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\p3pio\\OneDrive\\Desktop\\Machine Learning\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import gc\n",
    "import shutil\n",
    "import ijson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8db60d",
   "metadata": {},
   "source": [
    "## Document Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e456cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Loading documents...\n",
      "‚úÖ Loaded 233,027 documents\n",
      "üî™ Chunking 233027 documents with 4 strategies\n",
      "üìä Method: sentences\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Strategy: small\n",
      "  Chunk Size: 256 tokens\n",
      "  Overlap: 50 tokens\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking (small): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 233027/233027 [00:14<00:00, 15802.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Statistics for small:\n",
      "  Total Chunks: 233,078\n",
      "  Total Tokens: 9,198,884\n",
      "  Avg Tokens/Chunk: 39\n",
      "  Saved to: data/chunked\\chunks_small.json\n",
      "\n",
      "============================================================\n",
      "Strategy: medium\n",
      "  Chunk Size: 512 tokens\n",
      "  Overlap: 100 tokens\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking (medium): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 233027/233027 [00:20<00:00, 11648.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Statistics for medium:\n",
      "  Total Chunks: 233,032\n",
      "  Total Tokens: 9,196,824\n",
      "  Avg Tokens/Chunk: 39\n",
      "  Saved to: data/chunked\\chunks_medium.json\n",
      "\n",
      "============================================================\n",
      "Strategy: large\n",
      "  Chunk Size: 768 tokens\n",
      "  Overlap: 150 tokens\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking (large): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 233027/233027 [00:17<00:00, 13456.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Statistics for large:\n",
      "  Total Chunks: 233,028\n",
      "  Total Tokens: 9,196,473\n",
      "  Avg Tokens/Chunk: 39\n",
      "  Saved to: data/chunked\\chunks_large.json\n",
      "\n",
      "============================================================\n",
      "Strategy: extra_large\n",
      "  Chunk Size: 1024 tokens\n",
      "  Overlap: 200 tokens\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking (extra_large): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 233027/233027 [00:46<00:00, 5018.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Statistics for extra_large:\n",
      "  Total Chunks: 233,027\n",
      "  Total Tokens: 9,196,323\n",
      "  Avg Tokens/Chunk: 39\n",
      "  Saved to: data/chunked\\chunks_extra_large.json\n",
      "\n",
      "üìÑ Comparison report saved to: data/chunked\\chunking_report.json\n",
      "\n",
      "============================================================\n",
      "‚úÖ Chunking Complete!\n",
      "üìÇ All chunked data saved to: data/chunked/\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ChunkingStrategy:\n",
    "    \"\"\"Configuration for chunking strategy\"\"\"\n",
    "    name: str\n",
    "    chunk_size: int\n",
    "    chunk_overlap: int\n",
    "    \n",
    "class DocumentChunker:\n",
    "    \"\"\"\n",
    "    Splits documents into chunks for RAG pipeline testing\n",
    "    Tests multiple chunking strategies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encoding_name: str = \"cl100k_base\"):\n",
    "        \"\"\"\n",
    "        Initialize chunker\n",
    "        encoding_name: tiktoken encoding (cl100k_base for GPT-4, text-embedding-ada-002)\n",
    "        \"\"\"\n",
    "        self.encoding = tiktoken.get_encoding(encoding_name)\n",
    "        \n",
    "        # Define chunking strategies to test\n",
    "        self.strategies = [\n",
    "            ChunkingStrategy(name=\"small\", chunk_size=256, chunk_overlap=50),\n",
    "            ChunkingStrategy(name=\"medium\", chunk_size=512, chunk_overlap=100),\n",
    "            ChunkingStrategy(name=\"large\", chunk_size=768, chunk_overlap=150),\n",
    "            ChunkingStrategy(name=\"extra_large\", chunk_size=1024, chunk_overlap=200),\n",
    "        ]\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text\"\"\"\n",
    "        return len(self.encoding.encode(text))\n",
    "    \n",
    "    def split_by_tokens(self, text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into chunks by token count with overlap\n",
    "        \n",
    "        Args:\n",
    "            text: Text to split\n",
    "            chunk_size: Maximum tokens per chunk\n",
    "            overlap: Number of overlapping tokens between chunks\n",
    "        \n",
    "        Returns:\n",
    "            List of text chunks\n",
    "        \"\"\"\n",
    "        tokens = self.encoding.encode(text)\n",
    "        chunks = []\n",
    "        \n",
    "        start = 0\n",
    "        while start < len(tokens):\n",
    "            # Get chunk\n",
    "            end = start + chunk_size\n",
    "            chunk_tokens = tokens[start:end]\n",
    "            \n",
    "            # Decode back to text\n",
    "            chunk_text = self.encoding.decode(chunk_tokens)\n",
    "            chunks.append(chunk_text)\n",
    "            \n",
    "            # Move to next chunk with overlap\n",
    "            start = end - overlap\n",
    "            \n",
    "            # Prevent infinite loop\n",
    "            if start >= len(tokens) - overlap:\n",
    "                break\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def split_by_sentences(self, text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text by sentences, respecting token limits\n",
    "        Better for maintaining semantic coherence\n",
    "        \"\"\"\n",
    "        # Simple sentence splitting (can be improved with spaCy/nltk)\n",
    "        sentences = []\n",
    "        current = \"\"\n",
    "        \n",
    "        for char in text:\n",
    "            current += char\n",
    "            if char in '.!?' and len(current) > 20:  # Minimum sentence length\n",
    "                sentences.append(current.strip())\n",
    "                current = \"\"\n",
    "        \n",
    "        if current.strip():\n",
    "            sentences.append(current.strip())\n",
    "        \n",
    "        # Group sentences into chunks\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_tokens = self.count_tokens(sentence)\n",
    "            \n",
    "            # If single sentence exceeds chunk size, split it\n",
    "            if sentence_tokens > chunk_size:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                    current_chunk = \"\"\n",
    "                    current_tokens = 0\n",
    "                \n",
    "                # Split long sentence by tokens\n",
    "                token_chunks = self.split_by_tokens(sentence, chunk_size, overlap)\n",
    "                chunks.extend(token_chunks)\n",
    "                continue\n",
    "            \n",
    "            # Check if adding sentence exceeds chunk size\n",
    "            if current_tokens + sentence_tokens > chunk_size:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                \n",
    "                # Start new chunk with overlap\n",
    "                overlap_text = self.get_overlap_text(current_chunk, overlap)\n",
    "                current_chunk = overlap_text + \" \" + sentence\n",
    "                current_tokens = self.count_tokens(current_chunk)\n",
    "            else:\n",
    "                current_chunk += \" \" + sentence\n",
    "                current_tokens += sentence_tokens\n",
    "        \n",
    "        if current_chunk.strip():\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def get_overlap_text(self, text: str, overlap_tokens: int) -> str:\n",
    "        \"\"\"Get last N tokens from text for overlap\"\"\"\n",
    "        tokens = self.encoding.encode(text)\n",
    "        if len(tokens) <= overlap_tokens:\n",
    "            return text\n",
    "        \n",
    "        overlap_tokens_list = tokens[-overlap_tokens:]\n",
    "        return self.encoding.decode(overlap_tokens_list)\n",
    "    \n",
    "    def chunk_document(self, document: Dict, strategy: ChunkingStrategy, \n",
    "                       method: str = \"sentences\") -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Chunk a single document using specified strategy\n",
    "        \n",
    "        Args:\n",
    "            document: Document dict with 'content', 'title', etc.\n",
    "            strategy: ChunkingStrategy configuration\n",
    "            method: 'sentences' or 'tokens'\n",
    "        \n",
    "        Returns:\n",
    "            List of chunk dicts with metadata\n",
    "        \"\"\"\n",
    "        content = document.get('content', '')\n",
    "        \n",
    "        if not content or len(content.strip()) < 50:\n",
    "            return []\n",
    "        \n",
    "        # Choose splitting method\n",
    "        if method == \"sentences\":\n",
    "            chunks = self.split_by_sentences(\n",
    "                content, \n",
    "                strategy.chunk_size, \n",
    "                strategy.chunk_overlap\n",
    "            )\n",
    "        else:\n",
    "            chunks = self.split_by_tokens(\n",
    "                content,\n",
    "                strategy.chunk_size,\n",
    "                strategy.chunk_overlap\n",
    "            )\n",
    "        \n",
    "        # Create chunk objects with metadata\n",
    "        chunk_objects = []\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            chunk_obj = {\n",
    "                \"chunk_id\": f\"{document.get('id', 'doc')}_{strategy.name}_chunk_{i}\",\n",
    "                \"document_id\": document.get('id'),\n",
    "                \"chunk_index\": i,\n",
    "                \"total_chunks\": len(chunks),\n",
    "                \"text\": chunk_text,\n",
    "                \"token_count\": self.count_tokens(chunk_text),\n",
    "                \"strategy\": strategy.name,\n",
    "                \"chunk_size\": strategy.chunk_size,\n",
    "                \"overlap\": strategy.chunk_overlap,\n",
    "                \n",
    "                # Preserve document metadata\n",
    "                \"document_title\": document.get('title', ''),\n",
    "                \"document_type\": document.get('type', ''),\n",
    "                \"region\": document.get('region', ''),\n",
    "                \"province\": document.get('province', ''),\n",
    "                \"metadata\": document.get('metadata', {})\n",
    "            }\n",
    "            chunk_objects.append(chunk_obj)\n",
    "        \n",
    "        return chunk_objects\n",
    "    \n",
    "    def process_all_documents(self, documents: List[Dict], \n",
    "                             output_dir: str = \"data/chunked\",\n",
    "                             method: str = \"sentences\"):\n",
    "        \"\"\"\n",
    "        Process all documents with all chunking strategies\n",
    "        \n",
    "        Args:\n",
    "            documents: List of document dicts\n",
    "            output_dir: Directory to save chunked data\n",
    "            method: Chunking method ('sentences' or 'tokens')\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"üî™ Chunking {len(documents)} documents with {len(self.strategies)} strategies\")\n",
    "        print(f\"üìä Method: {method}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Process each strategy\n",
    "        for strategy in self.strategies:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Strategy: {strategy.name}\")\n",
    "            print(f\"  Chunk Size: {strategy.chunk_size} tokens\")\n",
    "            print(f\"  Overlap: {strategy.chunk_overlap} tokens\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            all_chunks = []\n",
    "            \n",
    "            for doc in tqdm(documents, desc=f\"Chunking ({strategy.name})\"):\n",
    "                chunks = self.chunk_document(doc, strategy, method)\n",
    "                all_chunks.extend(chunks)\n",
    "            \n",
    "            # Save chunks for this strategy\n",
    "            output_file = os.path.join(output_dir, f\"chunks_{strategy.name}.json\")\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            total_tokens = sum(c['token_count'] for c in all_chunks)\n",
    "            avg_tokens = total_tokens / len(all_chunks) if all_chunks else 0\n",
    "            \n",
    "            print(f\"\\nüìä Statistics for {strategy.name}:\")\n",
    "            print(f\"  Total Chunks: {len(all_chunks):,}\")\n",
    "            print(f\"  Total Tokens: {total_tokens:,}\")\n",
    "            print(f\"  Avg Tokens/Chunk: {avg_tokens:.0f}\")\n",
    "            print(f\"  Saved to: {output_file}\")\n",
    "        \n",
    "        # Generate comparison report\n",
    "        self.generate_comparison_report(output_dir)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"‚úÖ Chunking Complete!\")\n",
    "        print(f\"üìÇ All chunked data saved to: {output_dir}/\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    def generate_comparison_report(self, output_dir: str):\n",
    "        \"\"\"Generate a comparison report of all strategies\"\"\"\n",
    "        report = {\n",
    "            \"strategies\": [],\n",
    "            \"method\": \"sentence-based\"\n",
    "        }\n",
    "        \n",
    "        for strategy in self.strategies:\n",
    "            chunk_file = os.path.join(output_dir, f\"chunks_{strategy.name}.json\")\n",
    "            \n",
    "            if os.path.exists(chunk_file):\n",
    "                with open(chunk_file, 'r', encoding='utf-8') as f:\n",
    "                    chunks = json.load(f)\n",
    "                \n",
    "                strategy_stats = {\n",
    "                    \"name\": strategy.name,\n",
    "                    \"chunk_size\": strategy.chunk_size,\n",
    "                    \"overlap\": strategy.chunk_overlap,\n",
    "                    \"total_chunks\": len(chunks),\n",
    "                    \"total_tokens\": sum(c['token_count'] for c in chunks),\n",
    "                    \"avg_tokens_per_chunk\": sum(c['token_count'] for c in chunks) / len(chunks) if chunks else 0,\n",
    "                    \"file\": f\"chunks_{strategy.name}.json\"\n",
    "                }\n",
    "                report[\"strategies\"].append(strategy_stats)\n",
    "        \n",
    "        report_file = os.path.join(output_dir, \"chunking_report.json\")\n",
    "        with open(report_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nüìÑ Comparison report saved to: {report_file}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution\"\"\"\n",
    "    \n",
    "    # Load processed documents\n",
    "    train_file = \"data/processed/canada_wilderness_train.json\"\n",
    "    \n",
    "    if not os.path.exists(train_file):\n",
    "        print(f\"‚ùå Training data not found: {train_file}\")\n",
    "        print(\"Run combine_canada_data.py first!\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìñ Loading documents...\")\n",
    "    with open(train_file, 'r', encoding='utf-8') as f:\n",
    "        documents = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(documents):,} documents\")\n",
    "    \n",
    "    # Initialize chunker\n",
    "    chunker = DocumentChunker()\n",
    "    \n",
    "    # Process documents\n",
    "    # Note: For 277k documents, this will take time\n",
    "    # Consider processing a subset first for testing\n",
    "    \n",
    "    # # Option 1: Process subset for testing\n",
    "    # test_mode = input(\"\\nüß™ Test mode with 1000 documents? (y/n): \").lower().strip()\n",
    "    \n",
    "    # if test_mode == 'y':\n",
    "    #     print(\"üß™ Running in test mode with 1000 documents\")\n",
    "    #     documents = documents[:1000]\n",
    "    \n",
    "    # Process all documents\n",
    "    chunker.process_all_documents(\n",
    "        documents=documents,\n",
    "        output_dir=\"data/chunked\",\n",
    "        method=\"sentences\"  # Better for semantic coherence\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593170f9",
   "metadata": {},
   "source": [
    "## Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968117da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜì Free Embedding Generator (Local Models Only)\n",
      "============================================================\n",
      "\n",
      "Available chunking strategies:\n",
      "  1. small (256 tokens)\n",
      "  2. medium (512 tokens)\n",
      "  3. large (768 tokens)\n",
      "  4. extra_large (1024 tokens)\n",
      "  5. all strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Processing 1 strategy/strategies\n",
      "\n",
      "============================================================\n",
      "Processing: small\n",
      "============================================================\n",
      "üìä Loaded 233,078 chunks\n",
      "\n",
      "üîß Model: minilm\n",
      "  üì• Loading minilm...\n",
      "  üé® Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7284/7284 [56:15<00:00,  2.16it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Saved: data/embeddings\\small_minilm.json (2058.80 MB)\n",
      "\n",
      "üîß Model: bge_large\n",
      "  üì• Loading bge_large...\n",
      "  üé® Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 63/7284 [09:39<18:58:34,  9.46s/it]"
     ]
    }
   ],
   "source": [
    "class EmbeddingGenerator:\n",
    "    \"\"\"Generate embeddings using only free, local models  - without api keys\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.output_dir = \"data/embeddings\"\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        self.models = {\n",
    "            \"minilm\": \"all-MiniLM-L6-v2\",  # Fast, 384 dims\n",
    "            #\"bge_large\": \"BAAI/bge-large-en-v1.5\",  # High quality, 1024 dims\n",
    "        }\n",
    "        \n",
    "        self.loaded_models = {}\n",
    "    \n",
    "    def load_model(self, model_name: str):\n",
    "        \"\"\"Load model lazily\"\"\"\n",
    "        if model_name not in self.loaded_models:\n",
    "            print(f\"  üì• Loading {model_name}...\")\n",
    "            self.loaded_models[model_name] = SentenceTransformer(self.models[model_name])\n",
    "        return self.loaded_models[model_name]\n",
    "    \n",
    "    def generate_embeddings(self, chunks: List[Dict], model_key: str, \n",
    "                          batch_size: int = 32):\n",
    "        \"\"\"Generate embeddings for chunks\"\"\"\n",
    "        \n",
    "        model = self.load_model(model_key)\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        \n",
    "        print(f\"  üé® Generating embeddings...\")\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            embeddings = model.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
    "            all_embeddings.extend(embeddings.tolist())\n",
    "        \n",
    "        # Add embeddings to chunks\n",
    "        result = []\n",
    "        for chunk, embedding in zip(chunks, all_embeddings):\n",
    "            chunk_copy = chunk.copy()\n",
    "            chunk_copy['embedding'] = embedding\n",
    "            chunk_copy['embedding_model'] = model_key\n",
    "            chunk_copy['embedding_dimension'] = len(embedding)\n",
    "            result.append(chunk_copy)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def process_strategy(self, strategy_name: str):\n",
    "        \"\"\"Process one chunking strategy\"\"\"\n",
    "        \n",
    "        chunk_file = f\"data/chunked/chunks_{strategy_name}.json\"\n",
    "        \n",
    "        if not os.path.exists(chunk_file):\n",
    "            print(f\"‚ùå Not found: {chunk_file}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing: {strategy_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # FIX: Add encoding='utf-8'\n",
    "        with open(chunk_file, 'r', encoding='utf-8') as f:\n",
    "            chunks = json.load(f)\n",
    "        \n",
    "        print(f\"üìä Loaded {len(chunks):,} chunks\")\n",
    "        \n",
    "        # Process with each model\n",
    "        for model_key in self.models.keys():\n",
    "            print(f\"\\nüîß Model: {model_key}\")\n",
    "            \n",
    "            chunks_with_emb = self.generate_embeddings(chunks, model_key)\n",
    "            \n",
    "            # Save - FIX: Add encoding='utf-8'\n",
    "            output_file = os.path.join(\n",
    "                self.output_dir,\n",
    "                f\"{strategy_name}_{model_key}.json\"\n",
    "            )\n",
    "            \n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(chunks_with_emb, f, ensure_ascii=False)\n",
    "            \n",
    "            size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "            print(f\"  ‚úÖ Saved: {output_file} ({size_mb:.2f} MB)\")\n",
    "\n",
    "def main():\n",
    "    print(\"üÜì Free Embedding Generator (Local Models Only)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    generator = EmbeddingGenerator()\n",
    "    \n",
    "    # Ask user which strategy to process\n",
    "    print(\"\\nAvailable chunking strategies:\")\n",
    "    print(\"  1. small (256 tokens)\")\n",
    "    print(\"  2. medium (512 tokens)\")\n",
    "    print(\"  3. large (768 tokens)\")\n",
    "    print(\"  4. extra_large (1024 tokens)\")\n",
    "    print(\"  5. all strategies\")\n",
    "    \n",
    "    choice = input(\"\\nSelect strategy (1-5, default=1): \").strip() or '1'\n",
    "    \n",
    "    strategies_map = {\n",
    "        '1': ['small'],\n",
    "        '2': ['medium'],\n",
    "        '3': ['large'],\n",
    "        '4': ['extra_large'],\n",
    "        '5': ['small', 'medium', 'large', 'extra_large']\n",
    "    }\n",
    "    \n",
    "    strategies = strategies_map.get(choice, ['small'])\n",
    "    \n",
    "    print(f\"\\nüéØ Processing {len(strategies)} strategy/strategies\")\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        generator.process_strategy(strategy)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ Embedding generation complete!\")\n",
    "    print(f\"üìÇ Embeddings saved to: {generator.output_dir}/\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1fb6d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current embedding files: ['extra_large_minilm.json', 'medium_minilm.json', 'small_minilm.json']\n"
     ]
    }
   ],
   "source": [
    "#current files in embedding directory\n",
    "embedding_files = os.listdir(\"data/embeddings\")\n",
    "print(\"Current embedding files:\", embedding_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa99a70",
   "metadata": {},
   "source": [
    "## Vector Database Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1279fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üóÑÔ∏è  Organized Vector Database Setup\n",
      "======================================================================\n",
      "This will create a clean directory structure:\n",
      "  data/vector_db/\n",
      "    ‚îú‚îÄ‚îÄ small_minilm/\n",
      "    ‚îú‚îÄ‚îÄ medium_minilm/\n",
      "    ‚îî‚îÄ‚îÄ extra_large_minilm/\n",
      "======================================================================\n",
      "\n",
      "üóÑÔ∏è  Organized Vector Database Setup\n",
      "======================================================================\n",
      "\n",
      "üìä Found 3 embedding files:\n",
      "  - extra_large_minilm.json (2061.45 MB)\n",
      "  - medium_minilm.json (2059.05 MB)\n",
      "  - small_minilm.json (2058.8 MB)\n",
      "\n",
      "======================================================================\n",
      "Each collection will be stored in:\n",
      "  üìÅ data/vector_db/extra_large_minilm\n",
      "  üìÅ data/vector_db/medium_minilm\n",
      "  üìÅ data/vector_db/small_minilm\n",
      "\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Setting up: extra_large_minilm\n",
      "======================================================================\n",
      "  Chunk Strategy: extra\n",
      "  Embedding Model: large_minilm\n",
      "  File Size: 2061.45 MB\n",
      "  Directory: data/vector_db/extra_large_minilm\n",
      "  üìñ Loading: extra_large_minilm.json\n",
      "  ‚úÖ Loaded 233,027 chunks\n",
      "  üìä Embedding Dimension: 384\n",
      "  ‚úÖ Created collection\n",
      "  üíæ Adding 233,027 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 234/234 [18:31<00:00,  4.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Collection contains 233,027 items\n",
      "\n",
      "======================================================================\n",
      "Setting up: medium_minilm\n",
      "======================================================================\n",
      "  Chunk Strategy: medium\n",
      "  Embedding Model: minilm\n",
      "  File Size: 2059.05 MB\n",
      "  Directory: data/vector_db/medium_minilm\n",
      "  üìñ Loading: medium_minilm.json\n",
      "  ‚úÖ Loaded 233,032 chunks\n",
      "  üìä Embedding Dimension: 384\n",
      "  ‚úÖ Created collection\n",
      "  üíæ Adding 233,032 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 234/234 [17:45<00:00,  4.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Collection contains 233,032 items\n",
      "\n",
      "======================================================================\n",
      "Setting up: small_minilm\n",
      "======================================================================\n",
      "  Chunk Strategy: small\n",
      "  Embedding Model: minilm\n",
      "  File Size: 2058.8 MB\n",
      "  Directory: data/vector_db/small_minilm\n",
      "  üìñ Loading: small_minilm.json\n",
      "  ‚úÖ Loaded 233,078 chunks\n",
      "  üìä Embedding Dimension: 384\n",
      "  ‚úÖ Created collection\n",
      "  üíæ Adding 233,078 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 234/234 [08:08<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Collection contains 233,078 items\n",
      "\n",
      "======================================================================\n",
      "üìä SETUP SUMMARY\n",
      "======================================================================\n",
      "‚úÖ Successfully setup: 3 collections\n",
      "   - extra_large_minilm\n",
      "   - medium_minilm\n",
      "   - small_minilm\n",
      "\n",
      "üìÅ Directory Structure:\n",
      "data/vector_db//\n",
      "  ‚îú‚îÄ‚îÄ extra_large_minilm/\n",
      "  ‚îú‚îÄ‚îÄ medium_minilm/\n",
      "  ‚îú‚îÄ‚îÄ small_minilm/\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üìö All Collections:\n",
      "======================================================================\n",
      "  üìÅ extra_large_minilm/\n",
      "     ‚îî‚îÄ extra_large_minilm: 233,027 items\n",
      "  üìÅ medium_minilm/\n",
      "     ‚îî‚îÄ medium_minilm: 233,032 items\n",
      "  üìÅ small_minilm/\n",
      "     ‚îî‚îÄ small_minilm: 233,078 items\n",
      "\n",
      "  Total: 699,137 items across all collections\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "class VectorDatabaseManager:\n",
    "    \"\"\"\n",
    "    Manages vector database with organized directory structure\n",
    "    Each collection gets its own named directory\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_directory: str = \"data/vector_db/\"):\n",
    "        \"\"\"\n",
    "        Initialize manager\n",
    "        \n",
    "        Args:\n",
    "            base_directory: Base directory for all vector databases\n",
    "        \"\"\"\n",
    "        self.base_directory = base_directory\n",
    "        os.makedirs(base_directory, exist_ok=True)\n",
    "        \n",
    "        # Track available collections\n",
    "        self.available_embeddings = self.scan_available_embeddings()\n",
    "        self.collection_clients = {}  # Store separate clients per collection\n",
    "    \n",
    "    def scan_available_embeddings(self) -> List[Dict]:\n",
    "        \"\"\"Scan for available embedding files\"\"\"\n",
    "        \n",
    "        embedding_dir = \"data/embeddings\"\n",
    "        available = []\n",
    "        \n",
    "        if not os.path.exists(embedding_dir):\n",
    "            print(\"‚ö†Ô∏è  No embeddings directory found\")\n",
    "            return available\n",
    "        \n",
    "        for filename in os.listdir(embedding_dir):\n",
    "            if filename.endswith('.json') and not filename.endswith('_report.json'):\n",
    "                parts = filename.replace('.json', '').split('_')\n",
    "                \n",
    "                if len(parts) >= 2:\n",
    "                    chunk_strategy = parts[0]\n",
    "                    embedding_model = '_'.join(parts[1:])\n",
    "                    \n",
    "                    file_path = os.path.join(embedding_dir, filename)\n",
    "                    file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "                    \n",
    "                    collection_name = f\"{chunk_strategy}_{embedding_model}\"\n",
    "                    \n",
    "                    available.append({\n",
    "                        \"filename\": filename,\n",
    "                        \"filepath\": file_path,\n",
    "                        \"chunk_strategy\": chunk_strategy,\n",
    "                        \"embedding_model\": embedding_model,\n",
    "                        \"collection_name\": collection_name,\n",
    "                        \"file_size_mb\": round(file_size_mb, 2),\n",
    "                        \"db_directory\": os.path.join(self.base_directory, collection_name)\n",
    "                    })\n",
    "        \n",
    "        return available\n",
    "    \n",
    "    def get_client_for_collection(self, collection_name: str, db_directory: str):\n",
    "        \"\"\"\n",
    "        Get or create a ChromaDB client for a specific collection\n",
    "        Each collection gets its own directory\n",
    "        \"\"\"\n",
    "        \n",
    "        if collection_name not in self.collection_clients:\n",
    "            # Create directory for this collection\n",
    "            os.makedirs(db_directory, exist_ok=True)\n",
    "            \n",
    "            # Create dedicated client for this collection\n",
    "            client = chromadb.PersistentClient(path=db_directory)\n",
    "            self.collection_clients[collection_name] = client\n",
    "        \n",
    "        return self.collection_clients[collection_name]\n",
    "    \n",
    "    def load_embeddings_file(self, filepath: str) -> List[Dict]:\n",
    "        \"\"\"Load embeddings from JSON file\"\"\"\n",
    "        \n",
    "        print(f\"  üìñ Loading: {os.path.basename(filepath)}\")\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        print(f\"  ‚úÖ Loaded {len(data):,} chunks\")\n",
    "        return data\n",
    "    \n",
    "    def setup_collection(self, embedding_config: Dict):\n",
    "        \"\"\"\n",
    "        Setup a single collection in its own organized directory\n",
    "        \n",
    "        Args:\n",
    "            embedding_config: Dictionary with embedding file info\n",
    "        \"\"\"\n",
    "        \n",
    "        collection_name = embedding_config['collection_name']\n",
    "        db_directory = embedding_config['db_directory']\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Setting up: {collection_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"  Chunk Strategy: {embedding_config['chunk_strategy']}\")\n",
    "        print(f\"  Embedding Model: {embedding_config['embedding_model']}\")\n",
    "        print(f\"  File Size: {embedding_config['file_size_mb']} MB\")\n",
    "        print(f\"  Directory: {db_directory}\")\n",
    "        \n",
    "        # Load embeddings\n",
    "        chunks = self.load_embeddings_file(embedding_config['filepath'])\n",
    "        \n",
    "        if not chunks:\n",
    "            print(\"  ‚ùå No chunks loaded, skipping\")\n",
    "            return False\n",
    "        \n",
    "        # Get embedding dimension\n",
    "        embedding_dim = len(chunks[0]['embedding'])\n",
    "        print(f\"  üìä Embedding Dimension: {embedding_dim}\")\n",
    "        \n",
    "        # Get dedicated client for this collection\n",
    "        client = self.get_client_for_collection(collection_name, db_directory)\n",
    "        \n",
    "        # Delete existing collection if any\n",
    "        try:\n",
    "            client.delete_collection(name=collection_name)\n",
    "            print(f\"  üóëÔ∏è  Deleted existing collection\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Create collection\n",
    "        collection = client.create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        print(f\"  ‚úÖ Created collection\")\n",
    "        \n",
    "        # Add chunks in batches\n",
    "        print(f\"  üíæ Adding {len(chunks):,} chunks...\")\n",
    "        \n",
    "        batch_size = 1000\n",
    "        \n",
    "        for i in tqdm(range(0, len(chunks), batch_size), desc=\"  Progress\"):\n",
    "            batch = chunks[i:i + batch_size]\n",
    "            \n",
    "            ids = []\n",
    "            embeddings = []\n",
    "            documents = []\n",
    "            metadatas = []\n",
    "            \n",
    "            for chunk in batch:\n",
    "                ids.append(chunk['chunk_id'])\n",
    "                embeddings.append(chunk['embedding'])\n",
    "                documents.append(chunk['text'])\n",
    "                \n",
    "                metadata = {\n",
    "                    'document_id': chunk.get('document_id', ''),\n",
    "                    'document_title': chunk.get('document_title', ''),\n",
    "                    'document_type': chunk.get('document_type', ''),\n",
    "                    'region': chunk.get('region', ''),\n",
    "                    'province': chunk.get('province', ''),\n",
    "                    'chunk_index': chunk.get('chunk_index', 0),\n",
    "                    'total_chunks': chunk.get('total_chunks', 1),\n",
    "                    'token_count': chunk.get('token_count', 0),\n",
    "                    'strategy': chunk.get('strategy', ''),\n",
    "                    'embedding_model': chunk.get('embedding_model', '')\n",
    "                }\n",
    "                \n",
    "                if 'metadata' in chunk and isinstance(chunk['metadata'], dict):\n",
    "                    trail_meta = chunk['metadata']\n",
    "                    metadata['trail_type'] = trail_meta.get('trail_type', '')\n",
    "                    metadata['difficulty'] = trail_meta.get('difficulty', '')\n",
    "                    metadata['surface'] = trail_meta.get('surface', '')\n",
    "                \n",
    "                metadatas.append(metadata)\n",
    "            \n",
    "            try:\n",
    "                collection.add(\n",
    "                    ids=ids,\n",
    "                    embeddings=embeddings,\n",
    "                    documents=documents,\n",
    "                    metadatas=metadatas\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"\\n  ‚ö†Ô∏è  Error in batch {i//batch_size}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Verify\n",
    "        count = collection.count()\n",
    "        print(f\"  ‚úÖ Collection contains {count:,} items\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def setup_all_collections(self):\n",
    "        \"\"\"Setup all available collections\"\"\"\n",
    "        \n",
    "        print(\"üóÑÔ∏è  Organized Vector Database Setup\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if not self.available_embeddings:\n",
    "            print(\"‚ùå No embedding files found!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüìä Found {len(self.available_embeddings)} embedding files:\")\n",
    "        for emb in self.available_embeddings:\n",
    "            print(f\"  - {emb['filename']} ({emb['file_size_mb']} MB)\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"Each collection will be stored in:\")\n",
    "        for emb in self.available_embeddings:\n",
    "            print(f\"  üìÅ {emb['db_directory']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        proceed = input(\"\\nProceed with setup? (y/n): \").strip().lower()\n",
    "        \n",
    "        if proceed != 'y':\n",
    "            print(\"Cancelled\")\n",
    "            return\n",
    "        \n",
    "        # Setup all collections\n",
    "        successful = []\n",
    "        failed = []\n",
    "        \n",
    "        for emb_config in self.available_embeddings:\n",
    "            try:\n",
    "                if self.setup_collection(emb_config):\n",
    "                    successful.append(emb_config['collection_name'])\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Failed: {emb_config['collection_name']}\")\n",
    "                print(f\"   Error: {e}\")\n",
    "                failed.append(emb_config['collection_name'])\n",
    "        \n",
    "        # Summary\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"üìä SETUP SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"‚úÖ Successfully setup: {len(successful)} collections\")\n",
    "        for name in successful:\n",
    "            print(f\"   - {name}\")\n",
    "        \n",
    "        if failed:\n",
    "            print(f\"\\n‚ùå Failed: {len(failed)} collections\")\n",
    "            for name in failed:\n",
    "                print(f\"   - {name}\")\n",
    "        \n",
    "        print(f\"\\nüìÅ Directory Structure:\")\n",
    "        print(f\"{self.base_directory}/\")\n",
    "        for emb in self.available_embeddings:\n",
    "            if emb['collection_name'] in successful:\n",
    "                print(f\"  ‚îú‚îÄ‚îÄ {emb['collection_name']}/\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "    \n",
    "    def list_all_collections(self):\n",
    "        \"\"\"List all collections across all directories\"\"\"\n",
    "        \n",
    "        print(\"\\nüìö All Collections:\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if not os.path.exists(self.base_directory):\n",
    "            print(\"  No collections found\")\n",
    "            return\n",
    "        \n",
    "        total_items = 0\n",
    "        \n",
    "        for item in os.listdir(self.base_directory):\n",
    "            item_path = os.path.join(self.base_directory, item)\n",
    "            \n",
    "            if os.path.isdir(item_path):\n",
    "                try:\n",
    "                    client = chromadb.PersistentClient(path=item_path)\n",
    "                    collections = client.list_collections()\n",
    "                    \n",
    "                    for collection in collections:\n",
    "                        count = collection.count()\n",
    "                        total_items += count\n",
    "                        print(f\"  üìÅ {item}/\")\n",
    "                        print(f\"     ‚îî‚îÄ {collection.name}: {count:,} items\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ö†Ô∏è  {item}: Error loading ({e})\")\n",
    "        \n",
    "        print(f\"\\n  Total: {total_items:,} items across all collections\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution\"\"\"\n",
    "    \n",
    "    print(\"\\nüóÑÔ∏è  Organized Vector Database Setup\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"This will create a clean directory structure:\")\n",
    "    print(\"  data/vector_db/\")\n",
    "    print(\"    ‚îú‚îÄ‚îÄ small_minilm/\")\n",
    "    print(\"    ‚îú‚îÄ‚îÄ medium_minilm/\")\n",
    "    print(\"    ‚îî‚îÄ‚îÄ extra_large_minilm/\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    manager = VectorDatabaseManager()\n",
    "    \n",
    "    # Setup collections\n",
    "    manager.setup_all_collections()\n",
    "    \n",
    "    # List all collections\n",
    "    manager.list_all_collections()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb62ee29",
   "metadata": {},
   "source": [
    "## Vector-DB Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d044b094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Vector Database Test Suite (3 MiniLM Collections)\n",
      "\n",
      "Tests organized directory structure:\n",
      "  data/vector_db/small_minilm/\n",
      "  data/vector_db/medium_minilm/\n",
      "  data/vector_db/extra_large_minilm/\n",
      "\n",
      "Options:\n",
      "  1. Run full test suite (all queries, all collections)\n",
      "  2. Quick comparison test (1 query, all collections)\n",
      "  3. Test custom query on all collections\n",
      "üß™ Initializing Vector Database Tester\n",
      "======================================================================\n",
      "üì• Loading embedding model...\n",
      "  - Loading all-MiniLM-L6-v2 (384 dims)...\n",
      "‚úÖ Model loaded\n",
      "\n",
      "\n",
      "‚ö†Ô∏è  This will take ~5 minutes\n",
      "\n",
      "======================================================================\n",
      "üß™ COMPREHENSIVE TEST SUITE - ALL 3 COLLECTIONS\n",
      "======================================================================\n",
      "\n",
      "üìö Available Collections:\n",
      "----------------------------------------------------------------------\n",
      "  ‚úì small_minilm\n",
      "     Items: 233,078 | Dims: 384 | Model: MiniLM\n",
      "  ‚úì medium_minilm\n",
      "     Items: 233,032 | Dims: 384 | Model: MiniLM\n",
      "  ‚úì extra_large_minilm\n",
      "     Items: 233,027 | Dims: 384 | Model: MiniLM\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Test Query 1/7\n",
      "Category: location | Difficulty: easy\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üî¨ CROSS-COLLECTION COMPARISON\n",
      "======================================================================\n",
      "Query: 'What are hiking trails in British Columbia?'\n",
      "\n",
      "üìö Available Collections:\n",
      "----------------------------------------------------------------------\n",
      "  ‚úì small_minilm\n",
      "     Items: 233,078 | Dims: 384 | Model: MiniLM\n",
      "  ‚úì medium_minilm\n",
      "     Items: 233,032 | Dims: 384 | Model: MiniLM\n",
      "  ‚úì extra_large_minilm\n",
      "     Items: 233,027 | Dims: 384 | Model: MiniLM\n",
      "\n",
      "\n",
      "üîé Query: 'What are hiking trails in British Columbia?'\n",
      "   Collection: small_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 19.29ms\n",
      "   ‚è±Ô∏è  Query: 17.75ms\n",
      "   ‚è±Ô∏è  Total: 37.04ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. Summit Trail (Wilderness)\n",
      "      Similarity: 0.6924\n",
      "      Region: British Columbia South\n",
      "      Type: Trail\n",
      "      Preview: # Summit Trail (Wilderness)\n",
      "\n",
      "**Region**: British Columbia South, Canada\n",
      "\n",
      "\n",
      "Summit Trail (Wilderness) is a trail in Britis...\n",
      "\n",
      "   2. Fisher-Thunder-Park-Cascade Cross-Park Trek\n",
      "      Similarity: 0.6799\n",
      "      Region: British Columbia South\n",
      "      Type: Trail\n",
      "      Preview: # Fisher-Thunder-Park-Cascade Cross-Park Trek\n",
      "\n",
      "**Region**: British Columbia South, Canada\n",
      "\n",
      "\n",
      "Fisher-Thunder-Park-Cascade ...\n",
      "\n",
      "\n",
      "üîé Query: 'What are hiking trails in British Columbia?'\n",
      "   Collection: medium_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 7.00ms\n",
      "   ‚è±Ô∏è  Query: 224.42ms\n",
      "   ‚è±Ô∏è  Total: 231.41ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. Summit Trail (Wilderness)\n",
      "      Similarity: 0.6924\n",
      "      Region: British Columbia South\n",
      "      Type: Trail\n",
      "      Preview: # Summit Trail (Wilderness)\n",
      "\n",
      "**Region**: British Columbia South, Canada\n",
      "\n",
      "\n",
      "Summit Trail (Wilderness) is a trail in Britis...\n",
      "\n",
      "   2. Meeting of Waters Trail\n",
      "      Similarity: 0.6750\n",
      "      Region: British Columbia South\n",
      "      Type: Trail\n",
      "      Preview: # Meeting of Waters Trail\n",
      "\n",
      "**Region**: British Columbia South, Canada\n",
      "\n",
      "\n",
      "Meeting of Waters Trail is a trail in British Co...\n",
      "\n",
      "\n",
      "üîé Query: 'What are hiking trails in British Columbia?'\n",
      "   Collection: extra_large_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 7.11ms\n",
      "   ‚è±Ô∏è  Query: 255.88ms\n",
      "   ‚è±Ô∏è  Total: 262.99ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. Summit Trail (Wilderness)\n",
      "      Similarity: 0.6924\n",
      "      Region: British Columbia South\n",
      "      Type: Trail\n",
      "      Preview: # Summit Trail (Wilderness)\n",
      "\n",
      "**Region**: British Columbia South, Canada\n",
      "\n",
      "\n",
      "Summit Trail (Wilderness) is a trail in Britis...\n",
      "\n",
      "   2. Fisher-Thunder-Park-Cascade Cross-Park Trek\n",
      "      Similarity: 0.6799\n",
      "      Region: British Columbia South\n",
      "      Type: Trail\n",
      "      Preview: # Fisher-Thunder-Park-Cascade Cross-Park Trek\n",
      "\n",
      "**Region**: British Columbia South, Canada\n",
      "\n",
      "\n",
      "Fisher-Thunder-Park-Cascade ...\n",
      "\n",
      "======================================================================\n",
      "üìä COMPARISON SUMMARY\n",
      "======================================================================\n",
      "\n",
      "‚è±Ô∏è  Performance Metrics:\n",
      "----------------------------------------------------------------------\n",
      "  small_minilm              |  37.04ms\n",
      "  medium_minilm             | 231.41ms\n",
      "  extra_large_minilm        | 262.99ms\n",
      "\n",
      "üéØ Top Result Similarity:\n",
      "----------------------------------------------------------------------\n",
      "  small_minilm              | 0.6924 | Summit Trail (Wilderness)\n",
      "  medium_minilm             | 0.6924 | Summit Trail (Wilderness)\n",
      "  extra_large_minilm        | 0.6924 | Summit Trail (Wilderness)\n",
      "\n",
      "üèÜ Winners:\n",
      "----------------------------------------------------------------------\n",
      "  ‚ö° Fastest: small_minilm (37.04ms)\n",
      "  üéØ Most Accurate: small_minilm (0.6924)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Test Query 2/7\n",
      "Category: surface | Difficulty: medium\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üî¨ CROSS-COLLECTION COMPARISON\n",
      "======================================================================\n",
      "Query: 'Find trails with concrete surface in Quebec'\n",
      "\n",
      "üìö Available Collections:\n",
      "----------------------------------------------------------------------\n",
      "  ‚úì small_minilm\n",
      "     Items: 233,078 | Dims: 384 | Model: MiniLM\n",
      "  ‚úì medium_minilm\n",
      "     Items: 233,032 | Dims: 384 | Model: MiniLM\n",
      "  ‚úì extra_large_minilm\n",
      "     Items: 233,027 | Dims: 384 | Model: MiniLM\n",
      "\n",
      "\n",
      "üîé Query: 'Find trails with concrete surface in Quebec'\n",
      "   Collection: small_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 19.61ms\n",
      "   ‚è±Ô∏è  Query: 24.81ms\n",
      "   ‚è±Ô∏è  Total: 44.42ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. Boulevard des Laurentides\n",
      "      Similarity: 0.6465\n",
      "      Region: Quebec South\n",
      "      Type: Trail\n",
      "      Surface: concrete\n",
      "      Preview: # Boulevard des Laurentides\n",
      "\n",
      "**Region**: Quebec South, Canada\n",
      "\n",
      "\n",
      "Boulevard des Laurentides is a trail in Quebec South, Ca...\n",
      "\n",
      "   2. Boulevard des Laurentides\n",
      "      Similarity: 0.6465\n",
      "      Region: Quebec South\n",
      "      Type: Trail\n",
      "      Surface: concrete\n",
      "      Preview: # Boulevard des Laurentides\n",
      "\n",
      "**Region**: Quebec South, Canada\n",
      "\n",
      "\n",
      "Boulevard des Laurentides is a trail in Quebec South, Ca...\n",
      "\n",
      "\n",
      "üîé Query: 'Find trails with concrete surface in Quebec'\n",
      "   Collection: medium_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 10.58ms\n",
      "   ‚è±Ô∏è  Query: 238.99ms\n",
      "   ‚è±Ô∏è  Total: 249.57ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. Rue Charles-De La Tour\n",
      "      Similarity: 0.6669\n",
      "      Region: Quebec South\n",
      "      Type: Trail\n",
      "      Surface: concrete\n",
      "      Preview: # Rue Charles-De La Tour\n",
      "\n",
      "**Region**: Quebec South, Canada\n",
      "\n",
      "\n",
      "Rue Charles-De La Tour is a trail in Quebec South, Canada. ...\n",
      "\n",
      "   2. Rue Charles-De La Tour\n",
      "      Similarity: 0.6669\n",
      "      Region: Quebec South\n",
      "      Type: Trail\n",
      "      Surface: concrete\n",
      "      Preview: # Rue Charles-De La Tour\n",
      "\n",
      "**Region**: Quebec South, Canada\n",
      "\n",
      "\n",
      "Rue Charles-De La Tour is a trail in Quebec South, Canada. ...\n",
      "\n",
      "\n",
      "üîé Query: 'Find trails with concrete surface in Quebec'\n",
      "   Collection: extra_large_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 34.85ms\n",
      "   ‚è±Ô∏è  Query: 265.85ms\n",
      "   ‚è±Ô∏è  Total: 300.70ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. Rue Saint-Christophe\n",
      "      Similarity: 0.6512\n",
      "      Region: Quebec South\n",
      "      Type: Trail\n",
      "      Surface: concrete\n",
      "      Preview: # Rue Saint-Christophe\n",
      "\n",
      "**Region**: Quebec South, Canada\n",
      "\n",
      "\n",
      "Rue Saint-Christophe is a trail in Quebec South, Canada. It i...\n",
      "\n",
      "   2. Rue Saint-Christophe\n",
      "      Similarity: 0.6512\n",
      "      Region: Quebec South\n",
      "      Type: Trail\n",
      "      Surface: concrete\n",
      "      Preview: # Rue Saint-Christophe\n",
      "\n",
      "**Region**: Quebec South, Canada\n",
      "\n",
      "\n",
      "Rue Saint-Christophe is a trail in Quebec South, Canada. It i...\n",
      "\n",
      "======================================================================\n",
      "üìä COMPARISON SUMMARY\n",
      "======================================================================\n",
      "\n",
      "‚è±Ô∏è  Performance Metrics:\n",
      "----------------------------------------------------------------------\n",
      "  small_minilm              |  44.42ms\n",
      "  medium_minilm             | 249.57ms\n",
      "  extra_large_minilm        | 300.70ms\n",
      "\n",
      "üéØ Top Result Similarity:\n",
      "----------------------------------------------------------------------\n",
      "  small_minilm              | 0.6465 | Boulevard des Laurentides\n",
      "  medium_minilm             | 0.6669 | Rue Charles-De La Tour\n",
      "  extra_large_minilm        | 0.6512 | Rue Saint-Christophe\n",
      "\n",
      "üèÜ Winners:\n",
      "----------------------------------------------------------------------\n",
      "  ‚ö° Fastest: small_minilm (44.42ms)\n",
      "  üéØ Most Accurate: medium_minilm (0.6669)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Test Query 3/7\n",
      "Category: accessibility | Difficulty: easy\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üî¨ CROSS-COLLECTION COMPARISON\n",
      "======================================================================\n",
      "Query: 'Are there wheelchair accessible trails in Ontario?'\n",
      "\n",
      "üìö Available Collections:\n",
      "----------------------------------------------------------------------\n",
      "  ‚úì small_minilm\n",
      "     Items: 233,078 | Dims: 384 | Model: MiniLM\n",
      "  ‚úì medium_minilm\n",
      "     Items: 233,032 | Dims: 384 | Model: MiniLM\n",
      "  ‚úì extra_large_minilm\n",
      "     Items: 233,027 | Dims: 384 | Model: MiniLM\n",
      "\n",
      "\n",
      "üîé Query: 'Are there wheelchair accessible trails in Ontario?'\n",
      "   Collection: small_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 16.84ms\n",
      "   ‚è±Ô∏è  Query: 102.19ms\n",
      "   ‚è±Ô∏è  Total: 119.03ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. Wheelchairs only\n",
      "      Similarity: 0.6915\n",
      "      Region: Ontario South\n",
      "      Type: Trail\n",
      "      Preview: # Wheelchairs only\n",
      "\n",
      "**Region**: Ontario South, Canada\n",
      "\n",
      "\n",
      "Wheelchairs only is a trail in Ontario South, Canada. It is a fo...\n",
      "\n",
      "   2. Wheelchair loading\n",
      "      Similarity: 0.6792\n",
      "      Region: Ontario South\n",
      "      Type: Trail\n",
      "      Preview: # Wheelchair loading\n",
      "\n",
      "**Region**: Ontario South, Canada\n",
      "\n",
      "\n",
      "Wheelchair loading is a trail in Ontario South, Canada. It is ...\n",
      "\n",
      "\n",
      "üîé Query: 'Are there wheelchair accessible trails in Ontario?'\n",
      "   Collection: medium_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 23.66ms\n",
      "   ‚è±Ô∏è  Query: 474.82ms\n",
      "   ‚è±Ô∏è  Total: 498.47ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. Wheelchairs only\n",
      "      Similarity: 0.6915\n",
      "      Region: Ontario South\n",
      "      Type: Trail\n",
      "      Preview: # Wheelchairs only\n",
      "\n",
      "**Region**: Ontario South, Canada\n",
      "\n",
      "\n",
      "Wheelchairs only is a trail in Ontario South, Canada. It is a fo...\n",
      "\n",
      "   2. Wheelchair loading\n",
      "      Similarity: 0.6792\n",
      "      Region: Ontario South\n",
      "      Type: Trail\n",
      "      Preview: # Wheelchair loading\n",
      "\n",
      "**Region**: Ontario South, Canada\n",
      "\n",
      "\n",
      "Wheelchair loading is a trail in Ontario South, Canada. It is ...\n",
      "\n",
      "\n",
      "üîé Query: 'Are there wheelchair accessible trails in Ontario?'\n",
      "   Collection: extra_large_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 19.28ms\n",
      "   ‚è±Ô∏è  Query: 511.39ms\n",
      "   ‚è±Ô∏è  Total: 530.67ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. Wheelchairs only\n",
      "      Similarity: 0.6915\n",
      "      Region: Ontario South\n",
      "      Type: Trail\n",
      "      Preview: # Wheelchairs only\n",
      "\n",
      "**Region**: Ontario South, Canada\n",
      "\n",
      "\n",
      "Wheelchairs only is a trail in Ontario South, Canada. It is a fo...\n",
      "\n",
      "   2. Wheelchair loading\n",
      "      Similarity: 0.6792\n",
      "      Region: Ontario South\n",
      "      Type: Trail\n",
      "      Preview: # Wheelchair loading\n",
      "\n",
      "**Region**: Ontario South, Canada\n",
      "\n",
      "\n",
      "Wheelchair loading is a trail in Ontario South, Canada. It is ...\n",
      "\n",
      "======================================================================\n",
      "üìä COMPARISON SUMMARY\n",
      "======================================================================\n",
      "\n",
      "‚è±Ô∏è  Performance Metrics:\n",
      "----------------------------------------------------------------------\n",
      "  small_minilm              | 119.03ms\n",
      "  medium_minilm             | 498.47ms\n",
      "  extra_large_minilm        | 530.67ms\n",
      "\n",
      "üéØ Top Result Similarity:\n",
      "----------------------------------------------------------------------\n",
      "  small_minilm              | 0.6915 | Wheelchairs only\n",
      "  medium_minilm             | 0.6915 | Wheelchairs only\n",
      "  extra_large_minilm        | 0.6915 | Wheelchairs only\n",
      "\n",
      "üèÜ Winners:\n",
      "----------------------------------------------------------------------\n",
      "  ‚ö° Fastest: small_minilm (119.03ms)\n",
      "  üéØ Most Accurate: small_minilm (0.6915)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Test Query 4/7\n",
      "Category: activity | Difficulty: easy\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üî¨ CROSS-COLLECTION COMPARISON\n",
      "======================================================================\n",
      "Query: 'What trails allow bicycles?'\n",
      "\n",
      "üìö Available Collections:\n",
      "----------------------------------------------------------------------\n",
      "  ‚úì small_minilm\n",
      "     Items: 233,078 | Dims: 384 | Model: MiniLM\n",
      "  ‚úì medium_minilm\n",
      "     Items: 233,032 | Dims: 384 | Model: MiniLM\n",
      "  ‚úì extra_large_minilm\n",
      "     Items: 233,027 | Dims: 384 | Model: MiniLM\n",
      "\n",
      "\n",
      "üîé Query: 'What trails allow bicycles?'\n",
      "   Collection: small_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 37.97ms\n",
      "   ‚è±Ô∏è  Query: 55.00ms\n",
      "   ‚è±Ô∏è  Total: 92.96ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. I Will Allow It\n",
      "      Similarity: 0.6632\n",
      "      Region: Ontario South\n",
      "      Type: Trail\n",
      "      Surface: dirt_rock\n",
      "      Preview: # I Will Allow It\n",
      "\n",
      "**Region**: Ontario South, Canada\n",
      "\n",
      "\n",
      "I Will Allow It is a trail in Ontario South, Canada. It is a path...\n",
      "\n",
      "   2. Eazy Does It\n",
      "      Similarity: 0.6586\n",
      "      Region: Alberta South\n",
      "      Type: Trail\n",
      "      Preview: # Eazy Does It\n",
      "\n",
      "**Region**: Alberta South, Canada\n",
      "\n",
      "\n",
      "Eazy Does It is a trail in Alberta South, Canada. It is a path trail...\n",
      "\n",
      "\n",
      "üîé Query: 'What trails allow bicycles?'\n",
      "   Collection: medium_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 16.69ms\n",
      "   ‚è±Ô∏è  Query: 357.60ms\n",
      "   ‚è±Ô∏è  Total: 374.30ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. I Will Allow It\n",
      "      Similarity: 0.6632\n",
      "      Region: Ontario South\n",
      "      Type: Trail\n",
      "      Surface: dirt_rock\n",
      "      Preview: # I Will Allow It\n",
      "\n",
      "**Region**: Ontario South, Canada\n",
      "\n",
      "\n",
      "I Will Allow It is a trail in Ontario South, Canada. It is a path...\n",
      "\n",
      "   2. Eazy Does It\n",
      "      Similarity: 0.6586\n",
      "      Region: Alberta South\n",
      "      Type: Trail\n",
      "      Preview: # Eazy Does It\n",
      "\n",
      "**Region**: Alberta South, Canada\n",
      "\n",
      "\n",
      "Eazy Does It is a trail in Alberta South, Canada. It is a path trail...\n",
      "\n",
      "\n",
      "üîé Query: 'What trails allow bicycles?'\n",
      "   Collection: extra_large_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 20.05ms\n",
      "   ‚è±Ô∏è  Query: 625.43ms\n",
      "   ‚è±Ô∏è  Total: 645.48ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. I Will Allow It\n",
      "      Similarity: 0.6632\n",
      "      Region: Ontario South\n",
      "      Type: Trail\n",
      "      Surface: dirt_rock\n",
      "      Preview: # I Will Allow It\n",
      "\n",
      "**Region**: Ontario South, Canada\n",
      "\n",
      "\n",
      "I Will Allow It is a trail in Ontario South, Canada. It is a path...\n",
      "\n",
      "   2. Eazy Does It\n",
      "      Similarity: 0.6586\n",
      "      Region: Alberta South\n",
      "      Type: Trail\n",
      "      Preview: # Eazy Does It\n",
      "\n",
      "**Region**: Alberta South, Canada\n",
      "\n",
      "\n",
      "Eazy Does It is a trail in Alberta South, Canada. It is a path trail...\n",
      "\n",
      "======================================================================\n",
      "üìä COMPARISON SUMMARY\n",
      "======================================================================\n",
      "\n",
      "‚è±Ô∏è  Performance Metrics:\n",
      "----------------------------------------------------------------------\n",
      "  small_minilm              |  92.96ms\n",
      "  medium_minilm             | 374.30ms\n",
      "  extra_large_minilm        | 645.48ms\n",
      "\n",
      "üéØ Top Result Similarity:\n",
      "----------------------------------------------------------------------\n",
      "  small_minilm              | 0.6632 | I Will Allow It\n",
      "  medium_minilm             | 0.6632 | I Will Allow It\n",
      "  extra_large_minilm        | 0.6632 | I Will Allow It\n",
      "\n",
      "üèÜ Winners:\n",
      "----------------------------------------------------------------------\n",
      "  ‚ö° Fastest: small_minilm (92.96ms)\n",
      "  üéØ Most Accurate: small_minilm (0.6632)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Test Query 5/7\n",
      "Category: park | Difficulty: easy\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üî¨ CROSS-COLLECTION COMPARISON\n",
      "======================================================================\n",
      "Query: 'Tell me about Banff National Park'\n",
      "\n",
      "üìö Available Collections:\n",
      "----------------------------------------------------------------------\n",
      "  ‚úì small_minilm\n",
      "     Items: 233,078 | Dims: 384 | Model: MiniLM\n",
      "  ‚úì medium_minilm\n",
      "     Items: 233,032 | Dims: 384 | Model: MiniLM\n",
      "  ‚úì extra_large_minilm\n",
      "     Items: 233,027 | Dims: 384 | Model: MiniLM\n",
      "\n",
      "\n",
      "üîé Query: 'Tell me about Banff National Park'\n",
      "   Collection: small_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 11.54ms\n",
      "   ‚è±Ô∏è  Query: 24.96ms\n",
      "   ‚è±Ô∏è  Total: 36.49ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. Banff National Park\n",
      "      Similarity: 0.7007\n",
      "      Region: AB\n",
      "      Type: National Park\n",
      "      Preview: # Banff National Park\n",
      "**Type**: National Park\n",
      "**Province/Territory**: AB\n",
      "\n",
      "## Description\n",
      "Rocky Mountain peaks, glacial l...\n",
      "\n",
      "   2. Banff National Park\n",
      "      Similarity: 0.6705\n",
      "      Region: AB\n",
      "      Type: National Park\n",
      "      Preview: prohibited. Anyone caught operating a drone within park boundaries may result in law enforcement action and a fine of up...\n",
      "\n",
      "\n",
      "üîé Query: 'Tell me about Banff National Park'\n",
      "   Collection: medium_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 12.04ms\n",
      "   ‚è±Ô∏è  Query: 226.96ms\n",
      "   ‚è±Ô∏è  Total: 239.00ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. Banff National Park\n",
      "      Similarity: 0.7016\n",
      "      Region: AB\n",
      "      Type: National Park\n",
      "      Preview: # Banff National Park\n",
      "**Type**: National Park\n",
      "**Province/Territory**: AB\n",
      "\n",
      "## Description\n",
      "Rocky Mountain peaks, glacial l...\n",
      "\n",
      "   2. Wood Buffalo National Park\n",
      "      Similarity: 0.5364\n",
      "      Region: AB/NT\n",
      "      Type: National Park\n",
      "      Preview: Spanning the Alberta-Northwest Territories border, the largest national park in Canada protects a large swath of the Nor...\n",
      "\n",
      "\n",
      "üîé Query: 'Tell me about Banff National Park'\n",
      "   Collection: extra_large_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 16.67ms\n",
      "   ‚è±Ô∏è  Query: 233.93ms\n",
      "   ‚è±Ô∏è  Total: 250.61ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. Rue Banff\n",
      "      Similarity: 0.4892\n",
      "      Region: Quebec South\n",
      "      Type: Trail\n",
      "      Preview: # Rue Banff\n",
      "\n",
      "**Region**: Quebec South, Canada\n",
      "\n",
      "\n",
      "Rue Banff is a trail in Quebec South, Canada. It is a footway trail....\n",
      "\n",
      "   2. Rue Banting\n",
      "      Similarity: 0.4029\n",
      "      Region: Quebec South\n",
      "      Type: Trail\n",
      "      Surface: concrete\n",
      "      Preview: # Rue Banting\n",
      "\n",
      "**Region**: Quebec South, Canada\n",
      "\n",
      "\n",
      "Rue Banting is a trail in Quebec South, Canada. It is a footway trail....\n",
      "\n",
      "======================================================================\n",
      "üìä COMPARISON SUMMARY\n",
      "======================================================================\n",
      "\n",
      "‚è±Ô∏è  Performance Metrics:\n",
      "----------------------------------------------------------------------\n",
      "  small_minilm              |  36.49ms\n",
      "  medium_minilm             | 239.00ms\n",
      "  extra_large_minilm        | 250.61ms\n",
      "\n",
      "üéØ Top Result Similarity:\n",
      "----------------------------------------------------------------------\n",
      "  small_minilm              | 0.7007 | Banff National Park\n",
      "  medium_minilm             | 0.7016 | Banff National Park\n",
      "  extra_large_minilm        | 0.4892 | Rue Banff\n",
      "\n",
      "üèÜ Winners:\n",
      "----------------------------------------------------------------------\n",
      "  ‚ö° Fastest: small_minilm (36.49ms)\n",
      "  üéØ Most Accurate: medium_minilm (0.7016)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Test Query 6/7\n",
      "Category: difficulty | Difficulty: hard\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üî¨ CROSS-COLLECTION COMPARISON\n",
      "======================================================================\n",
      "Query: 'What are challenging mountain trails in Alberta?'\n",
      "\n",
      "üìö Available Collections:\n",
      "----------------------------------------------------------------------\n",
      "  ‚úì small_minilm\n",
      "     Items: 233,078 | Dims: 384 | Model: MiniLM\n",
      "  ‚úì medium_minilm\n",
      "     Items: 233,032 | Dims: 384 | Model: MiniLM\n",
      "  ‚úì extra_large_minilm\n",
      "     Items: 233,027 | Dims: 384 | Model: MiniLM\n",
      "\n",
      "\n",
      "üîé Query: 'What are challenging mountain trails in Alberta?'\n",
      "   Collection: small_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 46.01ms\n",
      "   ‚è±Ô∏è  Query: 50.87ms\n",
      "   ‚è±Ô∏è  Total: 96.89ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. Plateau Mountain Alternate Route\n",
      "      Similarity: 0.6602\n",
      "      Region: Alberta South\n",
      "      Type: Trail\n",
      "      Preview: # Plateau Mountain Alternate Route\n",
      "\n",
      "**Region**: Alberta South, Canada\n",
      "\n",
      "\n",
      "Plateau Mountain Alternate Route is a trail in A...\n",
      "\n",
      "   2. Plateau Mountain Alternate Route\n",
      "      Similarity: 0.6602\n",
      "      Region: Alberta South\n",
      "      Type: Trail\n",
      "      Preview: # Plateau Mountain Alternate Route\n",
      "\n",
      "**Region**: Alberta South, Canada\n",
      "\n",
      "\n",
      "Plateau Mountain Alternate Route is a trail in A...\n",
      "\n",
      "\n",
      "üîé Query: 'What are challenging mountain trails in Alberta?'\n",
      "   Collection: medium_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 11.32ms\n",
      "   ‚è±Ô∏è  Query: 271.39ms\n",
      "   ‚è±Ô∏è  Total: 282.71ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. Plateau Mountain Alternate Route\n",
      "      Similarity: 0.6602\n",
      "      Region: Alberta South\n",
      "      Type: Trail\n",
      "      Preview: # Plateau Mountain Alternate Route\n",
      "\n",
      "**Region**: Alberta South, Canada\n",
      "\n",
      "\n",
      "Plateau Mountain Alternate Route is a trail in A...\n",
      "\n",
      "   2. Plateau Mountain Alternate Route\n",
      "      Similarity: 0.6602\n",
      "      Region: Alberta South\n",
      "      Type: Trail\n",
      "      Preview: # Plateau Mountain Alternate Route\n",
      "\n",
      "**Region**: Alberta South, Canada\n",
      "\n",
      "\n",
      "Plateau Mountain Alternate Route is a trail in A...\n",
      "\n",
      "\n",
      "üîé Query: 'What are challenging mountain trails in Alberta?'\n",
      "   Collection: extra_large_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 17.71ms\n",
      "   ‚è±Ô∏è  Query: 404.68ms\n",
      "   ‚è±Ô∏è  Total: 422.39ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. Plateau Mountain Alternate Route\n",
      "      Similarity: 0.6602\n",
      "      Region: Alberta South\n",
      "      Type: Trail\n",
      "      Preview: # Plateau Mountain Alternate Route\n",
      "\n",
      "**Region**: Alberta South, Canada\n",
      "\n",
      "\n",
      "Plateau Mountain Alternate Route is a trail in A...\n",
      "\n",
      "   2. Plateau Mountain Alternate Route\n",
      "      Similarity: 0.6602\n",
      "      Region: Alberta South\n",
      "      Type: Trail\n",
      "      Preview: # Plateau Mountain Alternate Route\n",
      "\n",
      "**Region**: Alberta South, Canada\n",
      "\n",
      "\n",
      "Plateau Mountain Alternate Route is a trail in A...\n",
      "\n",
      "======================================================================\n",
      "üìä COMPARISON SUMMARY\n",
      "======================================================================\n",
      "\n",
      "‚è±Ô∏è  Performance Metrics:\n",
      "----------------------------------------------------------------------\n",
      "  small_minilm              |  96.89ms\n",
      "  medium_minilm             | 282.71ms\n",
      "  extra_large_minilm        | 422.39ms\n",
      "\n",
      "üéØ Top Result Similarity:\n",
      "----------------------------------------------------------------------\n",
      "  small_minilm              | 0.6602 | Plateau Mountain Alternate Rou\n",
      "  medium_minilm             | 0.6602 | Plateau Mountain Alternate Rou\n",
      "  extra_large_minilm        | 0.6602 | Plateau Mountain Alternate Rou\n",
      "\n",
      "üèÜ Winners:\n",
      "----------------------------------------------------------------------\n",
      "  ‚ö° Fastest: small_minilm (96.89ms)\n",
      "  üéØ Most Accurate: small_minilm (0.6602)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Test Query 7/7\n",
      "Category: multi-criteria | Difficulty: medium\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üî¨ CROSS-COLLECTION COMPARISON\n",
      "======================================================================\n",
      "Query: 'Find beginner-friendly trails near Toronto'\n",
      "\n",
      "üìö Available Collections:\n",
      "----------------------------------------------------------------------\n",
      "  ‚úì small_minilm\n",
      "     Items: 233,078 | Dims: 384 | Model: MiniLM\n",
      "  ‚úì medium_minilm\n",
      "     Items: 233,032 | Dims: 384 | Model: MiniLM\n",
      "  ‚úì extra_large_minilm\n",
      "     Items: 233,027 | Dims: 384 | Model: MiniLM\n",
      "\n",
      "\n",
      "üîé Query: 'Find beginner-friendly trails near Toronto'\n",
      "   Collection: small_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 10.99ms\n",
      "   ‚è±Ô∏è  Query: 5.69ms\n",
      "   ‚è±Ô∏è  Total: 16.68ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. L'explorateur boucle 1 et 2\n",
      "      Similarity: 0.4963\n",
      "      Region: Ontario Central\n",
      "      Type: Trail\n",
      "      Preview: # L'explorateur boucle 1 et 2\n",
      "\n",
      "**Region**: Ontario Central, Canada\n",
      "\n",
      "\n",
      "L'explorateur boucle 1 et 2 is a trail in Ontario C...\n",
      "\n",
      "   2. Boulevard De Maisonneuve Ouest\n",
      "      Similarity: 0.4799\n",
      "      Region: Quebec South\n",
      "      Type: Trail\n",
      "      Surface: concrete\n",
      "      Preview: # Boulevard De Maisonneuve Ouest\n",
      "\n",
      "**Region**: Quebec South, Canada\n",
      "\n",
      "\n",
      "Boulevard De Maisonneuve Ouest is a trail in Quebec...\n",
      "\n",
      "\n",
      "üîé Query: 'Find beginner-friendly trails near Toronto'\n",
      "   Collection: medium_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 14.87ms\n",
      "   ‚è±Ô∏è  Query: 221.51ms\n",
      "   ‚è±Ô∏è  Total: 236.38ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. Maple Trail\n",
      "      Similarity: 0.6295\n",
      "      Region: Ontario Central\n",
      "      Type: Trail\n",
      "      Difficulty: hiking\n",
      "      Surface: ground\n",
      "      Preview: # Maple Trail\n",
      "\n",
      "**Region**: Ontario Central, Canada\n",
      "\n",
      "\n",
      "Maple Trail is a trail in Ontario Central, Canada. It is a path tra...\n",
      "\n",
      "   2. Maple Trail\n",
      "      Similarity: 0.6171\n",
      "      Region: Ontario South\n",
      "      Type: Trail\n",
      "      Surface: grass\n",
      "      Preview: # Maple Trail\n",
      "\n",
      "**Region**: Ontario South, Canada\n",
      "\n",
      "\n",
      "Maple Trail is a trail in Ontario South, Canada. It is a footway trai...\n",
      "\n",
      "\n",
      "üîé Query: 'Find beginner-friendly trails near Toronto'\n",
      "   Collection: extra_large_minilm\n",
      "----------------------------------------------------------------------\n",
      "   ‚è±Ô∏è  Encoding: 24.32ms\n",
      "   ‚è±Ô∏è  Query: 273.24ms\n",
      "   ‚è±Ô∏è  Total: 297.56ms\n",
      "   ‚úÖ Retrieved 5 results\n",
      "\n",
      "   1. Maple Trail\n",
      "      Similarity: 0.6295\n",
      "      Region: Ontario Central\n",
      "      Type: Trail\n",
      "      Difficulty: hiking\n",
      "      Surface: ground\n",
      "      Preview: # Maple Trail\n",
      "\n",
      "**Region**: Ontario Central, Canada\n",
      "\n",
      "\n",
      "Maple Trail is a trail in Ontario Central, Canada. It is a path tra...\n",
      "\n",
      "   2. Maple Trail\n",
      "      Similarity: 0.6171\n",
      "      Region: Ontario South\n",
      "      Type: Trail\n",
      "      Surface: grass\n",
      "      Preview: # Maple Trail\n",
      "\n",
      "**Region**: Ontario South, Canada\n",
      "\n",
      "\n",
      "Maple Trail is a trail in Ontario South, Canada. It is a footway trai...\n",
      "\n",
      "======================================================================\n",
      "üìä COMPARISON SUMMARY\n",
      "======================================================================\n",
      "\n",
      "‚è±Ô∏è  Performance Metrics:\n",
      "----------------------------------------------------------------------\n",
      "  small_minilm              |  16.68ms\n",
      "  medium_minilm             | 236.38ms\n",
      "  extra_large_minilm        | 297.56ms\n",
      "\n",
      "üéØ Top Result Similarity:\n",
      "----------------------------------------------------------------------\n",
      "  small_minilm              | 0.4963 | L'explorateur boucle 1 et 2\n",
      "  medium_minilm             | 0.6295 | Maple Trail\n",
      "  extra_large_minilm        | 0.6295 | Maple Trail\n",
      "\n",
      "üèÜ Winners:\n",
      "----------------------------------------------------------------------\n",
      "  ‚ö° Fastest: small_minilm (16.68ms)\n",
      "  üéØ Most Accurate: medium_minilm (0.6295)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üìä FINAL COMPREHENSIVE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "‚è±Ô∏è  Average Performance by Collection:\n",
      "----------------------------------------------------------------------\n",
      "  small_minilm              | Avg:  63.36ms | Tests: 7\n",
      "  medium_minilm             | Avg: 301.69ms | Tests: 7\n",
      "  extra_large_minilm        | Avg: 387.20ms | Tests: 7\n",
      "\n",
      "üéØ Average Top Result Similarity:\n",
      "----------------------------------------------------------------------\n",
      "  small_minilm              | Avg Similarity: 0.6501\n",
      "  medium_minilm             | Avg Similarity: 0.6722\n",
      "  extra_large_minilm        | Avg Similarity: 0.6396\n",
      "\n",
      "üí° Recommendations for Your RAG Pipelines:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "  üöÄ Pipeline 1 (Speed-Optimized)\n",
      "     Collection: small_minilm\n",
      "     Avg time: 63.36ms\n",
      "     ‚Üí Best for: High-volume queries, real-time chat\n",
      "\n",
      "  üéØ Pipeline 2 (Quality-Optimized)\n",
      "     Collection: medium_minilm\n",
      "     Avg similarity: 0.6722\n",
      "     ‚Üí Best for: Complex queries, detailed answers\n",
      "\n",
      "  ‚öñÔ∏è  Pipeline 3 (Balanced)\n",
      "     Collection: small_minilm\n",
      "     Balance score: 0.902\n",
      "     ‚Üí Best for: Production use, general queries\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üíæ Test results saved to: data/evaluation\\vector_db_test_results.json\n"
     ]
    }
   ],
   "source": [
    "class VectorDBTester:\n",
    "    \"\"\"\n",
    "    Test all 3 MiniLM vector database collections\n",
    "    Works with organized directory structure\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_directory: str = \"data/vector_db/\"):\n",
    "        \"\"\"Initialize tester\"\"\"\n",
    "        \n",
    "        print(\"üß™ Initializing Vector Database Tester\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        self.base_directory = base_directory\n",
    "        \n",
    "        # Collections we have\n",
    "        self.collections = [\n",
    "            \"small_minilm\",\n",
    "            \"medium_minilm\",\n",
    "            \"extra_large_minilm\"\n",
    "        ]\n",
    "        \n",
    "        # Store clients for each collection\n",
    "        self.clients = {}\n",
    "        \n",
    "        # Load embedding model (only MiniLM)\n",
    "        print(\"üì• Loading embedding model...\")\n",
    "        print(\"  - Loading all-MiniLM-L6-v2 (384 dims)...\")\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"‚úÖ Model loaded\\n\")\n",
    "        \n",
    "        # Test queries\n",
    "        self.test_queries = [\n",
    "            {\n",
    "                \"query\": \"What are hiking trails in British Columbia?\",\n",
    "                \"category\": \"location\",\n",
    "                \"difficulty\": \"easy\"\n",
    "            },\n",
    "            {\n",
    "                \"query\": \"Find trails with concrete surface in Quebec\",\n",
    "                \"category\": \"surface\",\n",
    "                \"difficulty\": \"medium\"\n",
    "            },\n",
    "            {\n",
    "                \"query\": \"Are there wheelchair accessible trails in Ontario?\",\n",
    "                \"category\": \"accessibility\",\n",
    "                \"difficulty\": \"easy\"\n",
    "            },\n",
    "            {\n",
    "                \"query\": \"What trails allow bicycles?\",\n",
    "                \"category\": \"activity\",\n",
    "                \"difficulty\": \"easy\"\n",
    "            },\n",
    "            {\n",
    "                \"query\": \"Tell me about Banff National Park\",\n",
    "                \"category\": \"park\",\n",
    "                \"difficulty\": \"easy\"\n",
    "            },\n",
    "            {\n",
    "                \"query\": \"What are challenging mountain trails in Alberta?\",\n",
    "                \"category\": \"difficulty\",\n",
    "                \"difficulty\": \"hard\"\n",
    "            },\n",
    "            {\n",
    "                \"query\": \"Find beginner-friendly trails near Toronto\",\n",
    "                \"category\": \"multi-criteria\",\n",
    "                \"difficulty\": \"medium\"\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def get_client_for_collection(self, collection_name: str):\n",
    "        \"\"\"Get ChromaDB client for a specific collection\"\"\"\n",
    "        \n",
    "        if collection_name not in self.clients:\n",
    "            collection_path = os.path.join(self.base_directory, collection_name)\n",
    "            \n",
    "            if not os.path.exists(collection_path):\n",
    "                raise FileNotFoundError(f\"Collection directory not found: {collection_path}\")\n",
    "            \n",
    "            self.clients[collection_name] = chromadb.PersistentClient(path=collection_path)\n",
    "        \n",
    "        return self.clients[collection_name]\n",
    "    \n",
    "    def list_all_collections(self) -> List[str]:\n",
    "        \"\"\"List all available collections\"\"\"\n",
    "        \n",
    "        print(\"üìö Available Collections:\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        available = []\n",
    "        \n",
    "        for collection_name in self.collections:\n",
    "            collection_path = os.path.join(self.base_directory, collection_name)\n",
    "            \n",
    "            if not os.path.exists(collection_path):\n",
    "                print(f\"  ‚ö†Ô∏è  {collection_name}: Directory not found\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                client = self.get_client_for_collection(collection_name)\n",
    "                collections = client.list_collections()\n",
    "                \n",
    "                for collection in collections:\n",
    "                    count = collection.count()\n",
    "                    \n",
    "                    # Get embedding dimension\n",
    "                    sample = collection.peek(limit=1)\n",
    "                    if sample['embeddings'] is not None and len(sample['embeddings']) > 0:\n",
    "                        dim = len(sample['embeddings'][0])\n",
    "                    else:\n",
    "                        dim = 'unknown'\n",
    "                    \n",
    "                    print(f\"  ‚úì {collection_name}\")\n",
    "                    print(f\"     Items: {count:,} | Dims: {dim} | Model: MiniLM\")\n",
    "                    \n",
    "                    available.append(collection_name)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è  {collection_name}: Error loading ({e})\")\n",
    "        \n",
    "        print()\n",
    "        return available\n",
    "    \n",
    "    def test_single_query(self, collection_name: str, query_text: str, \n",
    "                         top_k: int = 5) -> Dict:\n",
    "        \"\"\"Test a single query\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Get client for this collection\n",
    "            client = self.get_client_for_collection(collection_name)\n",
    "            \n",
    "            # Get collection\n",
    "            collection = client.get_collection(name=collection_name)\n",
    "            \n",
    "            # Encode query\n",
    "            start_time = time.time()\n",
    "            query_embedding = self.embedding_model.encode(query_text, convert_to_numpy=True)\n",
    "            encode_time = time.time() - start_time\n",
    "            \n",
    "            # Query collection\n",
    "            start_time = time.time()\n",
    "            results = collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k,\n",
    "                include=['documents', 'metadatas', 'distances']\n",
    "            )\n",
    "            query_time = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"results\": results,\n",
    "                \"encode_time\": encode_time,\n",
    "                \"query_time\": query_time,\n",
    "                \"model_used\": \"minilm\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def display_query_results(self, query_text: str, collection_name: str, \n",
    "                             result: Dict, show_top_n: int = 3):\n",
    "        \"\"\"Display query results\"\"\"\n",
    "        \n",
    "        print(f\"\\nüîé Query: '{query_text}'\")\n",
    "        print(f\"   Collection: {collection_name}\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        if not result['success']:\n",
    "            print(f\"   ‚ùå Error: {result.get('error', 'Unknown error')}\\n\")\n",
    "            return\n",
    "        \n",
    "        print(f\"   ‚è±Ô∏è  Encoding: {result['encode_time']*1000:.2f}ms\")\n",
    "        print(f\"   ‚è±Ô∏è  Query: {result['query_time']*1000:.2f}ms\")\n",
    "        print(f\"   ‚è±Ô∏è  Total: {(result['encode_time'] + result['query_time'])*1000:.2f}ms\")\n",
    "        print(f\"   ‚úÖ Retrieved {len(result['results']['ids'][0])} results\\n\")\n",
    "        \n",
    "        # Show top results\n",
    "        for i in range(min(show_top_n, len(result['results']['ids'][0]))):\n",
    "            metadata = result['results']['metadatas'][0][i]\n",
    "            document = result['results']['documents'][0][i]\n",
    "            distance = result['results']['distances'][0][i]\n",
    "            similarity = 1 - distance\n",
    "            \n",
    "            print(f\"   {i+1}. {metadata.get('document_title', 'Unknown')}\")\n",
    "            print(f\"      Similarity: {similarity:.4f}\")\n",
    "            print(f\"      Region: {metadata.get('region', 'N/A')}\")\n",
    "            print(f\"      Type: {metadata.get('document_type', 'N/A')}\")\n",
    "            \n",
    "            if metadata.get('difficulty') and metadata.get('difficulty') != 'unknown':\n",
    "                print(f\"      Difficulty: {metadata.get('difficulty')}\")\n",
    "            if metadata.get('surface') and metadata.get('surface') != 'unknown':\n",
    "                print(f\"      Surface: {metadata.get('surface')}\")\n",
    "            \n",
    "            print(f\"      Preview: {document[:120]}...\")\n",
    "            print()\n",
    "    \n",
    "    def compare_all_collections(self, query_text: str, top_k: int = 5):\n",
    "        \"\"\"Compare same query across all 3 collections\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üî¨ CROSS-COLLECTION COMPARISON\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Query: '{query_text}'\\n\")\n",
    "        \n",
    "        collection_names = self.list_all_collections()\n",
    "        \n",
    "        if not collection_names:\n",
    "            print(\"‚ùå No collections available!\")\n",
    "            return {}\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for collection_name in collection_names:\n",
    "            result = self.test_single_query(collection_name, query_text, top_k)\n",
    "            all_results[collection_name] = result\n",
    "            self.display_query_results(query_text, collection_name, result, show_top_n=2)\n",
    "        \n",
    "        # Comparative summary\n",
    "        print(f\"{'='*70}\")\n",
    "        print(\"üìä COMPARISON SUMMARY\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        # Performance comparison\n",
    "        print(\"‚è±Ô∏è  Performance Metrics:\")\n",
    "        print(\"-\"*70)\n",
    "        for name, result in all_results.items():\n",
    "            if result['success']:\n",
    "                total_time = (result['encode_time'] + result['query_time']) * 1000\n",
    "                print(f\"  {name:25s} | {total_time:6.2f}ms\")\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        print(f\"\\nüéØ Top Result Similarity:\")\n",
    "        print(\"-\"*70)\n",
    "        for name, result in all_results.items():\n",
    "            if result['success'] and result['results']['distances'][0]:\n",
    "                top_similarity = 1 - result['results']['distances'][0][0]\n",
    "                top_title = result['results']['metadatas'][0][0].get('document_title', 'Unknown')\n",
    "                print(f\"  {name:25s} | {top_similarity:.4f} | {top_title[:30]}\")\n",
    "        \n",
    "        # Winners\n",
    "        print(f\"\\nüèÜ Winners:\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        # Fastest\n",
    "        fastest = min(all_results.items(), \n",
    "                     key=lambda x: (x[1]['encode_time'] + x[1]['query_time']) if x[1]['success'] else float('inf'))\n",
    "        fastest_time = (fastest[1]['encode_time'] + fastest[1]['query_time']) * 1000\n",
    "        print(f\"  ‚ö° Fastest: {fastest[0]} ({fastest_time:.2f}ms)\")\n",
    "        \n",
    "        # Most accurate\n",
    "        highest_sim = max(all_results.items(),\n",
    "                         key=lambda x: (1 - x[1]['results']['distances'][0][0]) if x[1]['success'] and x[1]['results']['distances'][0] else 0)\n",
    "        highest_score = 1 - highest_sim[1]['results']['distances'][0][0] if highest_sim[1]['success'] else 0\n",
    "        print(f\"  üéØ Most Accurate: {highest_sim[0]} ({highest_score:.4f})\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def run_comprehensive_test_suite(self):\n",
    "        \"\"\"Run tests on all collections with all queries\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"üß™ COMPREHENSIVE TEST SUITE - ALL 3 COLLECTIONS\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        collection_names = self.list_all_collections()\n",
    "        \n",
    "        if not collection_names:\n",
    "            print(\"‚ùå No collections found!\")\n",
    "            return\n",
    "        \n",
    "        # Summary results\n",
    "        summary = {'queries': {}}\n",
    "        \n",
    "        # Test each query\n",
    "        for i, test_query in enumerate(self.test_queries, 1):\n",
    "            query_text = test_query['query']\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"Test Query {i}/{len(self.test_queries)}\")\n",
    "            print(f\"Category: {test_query['category']} | Difficulty: {test_query['difficulty']}\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            query_results = self.compare_all_collections(query_text, top_k=5)\n",
    "            summary['queries'][query_text] = query_results\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        # Generate final summary\n",
    "        self.generate_final_summary(summary, collection_names)\n",
    "    \n",
    "    def generate_final_summary(self, summary: Dict, collection_names: List[str]):\n",
    "        \"\"\"Generate final comprehensive summary\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"üìä FINAL COMPREHENSIVE SUMMARY\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        # Average performance\n",
    "        print(\"‚è±Ô∏è  Average Performance by Collection:\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        for collection in collection_names:\n",
    "            total_time = 0\n",
    "            count = 0\n",
    "            \n",
    "            for query_text, results in summary['queries'].items():\n",
    "                if collection in results and results[collection]['success']:\n",
    "                    total_time += (results[collection]['encode_time'] + \n",
    "                                 results[collection]['query_time']) * 1000\n",
    "                    count += 1\n",
    "            \n",
    "            avg_time = total_time / count if count > 0 else 0\n",
    "            print(f\"  {collection:25s} | Avg: {avg_time:6.2f}ms | Tests: {count}\")\n",
    "        \n",
    "        # Average similarity\n",
    "        print(f\"\\nüéØ Average Top Result Similarity:\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        for collection in collection_names:\n",
    "            total_sim = 0\n",
    "            count = 0\n",
    "            \n",
    "            for query_text, results in summary['queries'].items():\n",
    "                if collection in results and results[collection]['success']:\n",
    "                    if results[collection]['results']['distances'][0]:\n",
    "                        similarity = 1 - results[collection]['results']['distances'][0][0]\n",
    "                        total_sim += similarity\n",
    "                        count += 1\n",
    "            \n",
    "            avg_sim = total_sim / count if count > 0 else 0\n",
    "            print(f\"  {collection:25s} | Avg Similarity: {avg_sim:.4f}\")\n",
    "        \n",
    "        # Recommendations\n",
    "        print(f\"\\nüí° Recommendations for Your RAG Pipelines:\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        # Speed champion\n",
    "        fastest_times = {}\n",
    "        for collection in collection_names:\n",
    "            total_time = 0\n",
    "            count = 0\n",
    "            for query_text, results in summary['queries'].items():\n",
    "                if collection in results and results[collection]['success']:\n",
    "                    total_time += (results[collection]['encode_time'] + \n",
    "                                 results[collection]['query_time'])\n",
    "                    count += 1\n",
    "            fastest_times[collection] = total_time / count if count > 0 else float('inf')\n",
    "        \n",
    "        fastest = min(fastest_times.items(), key=lambda x: x[1])\n",
    "        print(f\"\\n  üöÄ Pipeline 1 (Speed-Optimized)\")\n",
    "        print(f\"     Collection: {fastest[0]}\")\n",
    "        print(f\"     Avg time: {fastest[1]*1000:.2f}ms\")\n",
    "        print(f\"     ‚Üí Best for: High-volume queries, real-time chat\")\n",
    "        \n",
    "        # Accuracy champion\n",
    "        accuracy_scores = {}\n",
    "        for collection in collection_names:\n",
    "            total_sim = 0\n",
    "            count = 0\n",
    "            for query_text, results in summary['queries'].items():\n",
    "                if collection in results and results[collection]['success']:\n",
    "                    if results[collection]['results']['distances'][0]:\n",
    "                        total_sim += 1 - results[collection]['results']['distances'][0][0]\n",
    "                        count += 1\n",
    "            accuracy_scores[collection] = total_sim / count if count > 0 else 0\n",
    "        \n",
    "        most_accurate = max(accuracy_scores.items(), key=lambda x: x[1])\n",
    "        print(f\"\\n  üéØ Pipeline 2 (Quality-Optimized)\")\n",
    "        print(f\"     Collection: {most_accurate[0]}\")\n",
    "        print(f\"     Avg similarity: {most_accurate[1]:.4f}\")\n",
    "        print(f\"     ‚Üí Best for: Complex queries, detailed answers\")\n",
    "        \n",
    "        # Balanced\n",
    "        balanced_scores = {}\n",
    "        for collection in collection_names:\n",
    "            if collection in fastest_times and collection in accuracy_scores:\n",
    "                # Normalize and combine\n",
    "                norm_time = 1 - (fastest_times[collection] / max(fastest_times.values()))\n",
    "                norm_acc = accuracy_scores[collection] / max(accuracy_scores.values())\n",
    "                balanced_scores[collection] = (norm_time + norm_acc) / 2\n",
    "        \n",
    "        if balanced_scores:\n",
    "            balanced = max(balanced_scores.items(), key=lambda x: x[1])\n",
    "            print(f\"\\n  ‚öñÔ∏è  Pipeline 3 (Balanced)\")\n",
    "            print(f\"     Collection: {balanced[0]}\")\n",
    "            print(f\"     Balance score: {balanced[1]:.3f}\")\n",
    "            print(f\"     ‚Üí Best for: Production use, general queries\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\\n\")\n",
    "        \n",
    "        # Save results\n",
    "        self.save_results(summary, fastest_times, accuracy_scores)\n",
    "    \n",
    "    def save_results(self, summary: Dict, speed_scores: Dict, accuracy_scores: Dict):\n",
    "        \"\"\"Save test results\"\"\"\n",
    "        \n",
    "        output_dir = \"data/evaluation\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        results = {\n",
    "            \"test_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"collections_tested\": list(speed_scores.keys()),\n",
    "            \"total_queries\": len(summary['queries']),\n",
    "            \"performance_scores\": {\n",
    "                \"speed_ms\": {k: round(v*1000, 2) for k, v in speed_scores.items()},\n",
    "                \"accuracy\": {k: round(v, 4) for k, v in accuracy_scores.items()}\n",
    "            },\n",
    "            \"winners\": {\n",
    "                \"fastest\": min(speed_scores.items(), key=lambda x: x[1])[0],\n",
    "                \"most_accurate\": max(accuracy_scores.items(), key=lambda x: x[1])[0]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        output_file = os.path.join(output_dir, \"vector_db_test_results.json\")\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"üíæ Test results saved to: {output_file}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution\"\"\"\n",
    "    \n",
    "    print(\"\\nüß™ Vector Database Test Suite (3 MiniLM Collections)\\n\")\n",
    "    print(\"Tests organized directory structure:\")\n",
    "    print(\"  data/vector_db/small_minilm/\")\n",
    "    print(\"  data/vector_db/medium_minilm/\")\n",
    "    print(\"  data/vector_db/extra_large_minilm/\\n\")\n",
    "    print(\"Options:\")\n",
    "    print(\"  1. Run full test suite (all queries, all collections)\")\n",
    "    print(\"  2. Quick comparison test (1 query, all collections)\")\n",
    "    print(\"  3. Test custom query on all collections\")\n",
    "    \n",
    "    choice = input(\"\\nSelect option (1-3, default=2): \").strip() or '2'\n",
    "    \n",
    "    tester = VectorDBTester()\n",
    "    \n",
    "    if choice == '1':\n",
    "        # Full test suite\n",
    "        print(\"\\n‚ö†Ô∏è  This will take ~5 minutes\")\n",
    "        confirm = input(\"Continue? (y/n): \").strip().lower()\n",
    "        if confirm == 'y':\n",
    "            tester.run_comprehensive_test_suite()\n",
    "        else:\n",
    "            print(\"Cancelled\")\n",
    "    \n",
    "    elif choice == '2':\n",
    "        # Quick comparison\n",
    "        query = \"What are the best hiking trails in British Columbia?\"\n",
    "        tester.compare_all_collections(query, top_k=5)\n",
    "    \n",
    "    elif choice == '3':\n",
    "        # Custom query\n",
    "        query = input(\"\\nEnter your query: \").strip()\n",
    "        if query:\n",
    "            tester.compare_all_collections(query, top_k=5)\n",
    "        else:\n",
    "            print(\"No query provided\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
